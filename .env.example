# Claude Context Docker Environment Configuration
# 
# This file is OPTIONAL for the default Docker setup!
# The docker-compose.yml already configures Ollama + Qdrant with sensible defaults.
# 
# Only create a .env file if you want to customize the default behavior.

# =============================================================================
# CUSTOMIZATION OPTIONS (All optional)
# =============================================================================

# Custom file extensions to index beyond defaults (comma-separated, include the dot)
# Example: CUSTOM_EXTENSIONS=.vue,.svelte,.astro,.razor
# CUSTOM_EXTENSIONS=

# Custom ignore patterns beyond defaults (comma-separated) 
# Example: CUSTOM_IGNORE_PATTERNS=static/**,*.tmp,private/**,uploads/**
# CUSTOM_IGNORE_PATTERNS=

# Embedding batch size for processing (default: 100)
# Reduce this if you have memory constraints
# EMBEDDING_BATCH_SIZE=50

# =============================================================================
# SWITCH TO CLOUD PROVIDERS (requires API keys)
# =============================================================================

# To use OpenAI instead of local Ollama:
# EMBEDDING_PROVIDER=OpenAI
# OPENAI_API_KEY=sk-your-openai-api-key-here

# To use VoyageAI instead of local Ollama:  
# EMBEDDING_PROVIDER=VoyageAI
# VOYAGEAI_API_KEY=pa-your-voyageai-api-key-here

# To use Gemini instead of local Ollama:
# EMBEDDING_PROVIDER=Gemini
# GEMINI_API_KEY=your-gemini-api-key-here

# To use Qdrant Cloud instead of local Qdrant:
# QDRANT_URL=https://your-cluster.qdrant.io
# QDRANT_API_KEY=your-qdrant-api-key-here

# To use Zilliz Cloud instead of local Qdrant:
# VECTOR_DATABASE=Milvus
# MILVUS_TOKEN=your-zilliz-cloud-api-key

# =============================================================================
# ADVANCED PERFORMANCE TUNING
# =============================================================================

# Memory limits for containers (uncomment to set)
# OLLAMA_MEMORY_LIMIT=4g
# QDRANT_MEMORY_LIMIT=2g
# MCP_MEMORY_LIMIT=1g

# Use different Ollama model (requires pulling the model first)
# OLLAMA_MODEL=all-minilm  # Smaller, faster
# OLLAMA_MODEL=codellama:7b-code  # Better for code understanding
